{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 8.942091\n",
      "Train Epoch: 1 [1000/60000 (2%)]\tLoss: 6.461896\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 4.969244\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 4.038438\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 3.380217\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 3.035842\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 2.784003\n",
      "Train Epoch: 1 [7000/60000 (12%)]\tLoss: 2.656564\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 2.569643\n",
      "Train Epoch: 1 [9000/60000 (15%)]\tLoss: 2.336336\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 2.266963\n",
      "Train Epoch: 1 [11000/60000 (18%)]\tLoss: 2.324326\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 2.229839\n",
      "Train Epoch: 1 [13000/60000 (22%)]\tLoss: 2.191714\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 2.105567\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 2.050247\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.011894\n",
      "Train Epoch: 1 [17000/60000 (28%)]\tLoss: 2.082092\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 2.004577\n",
      "Train Epoch: 1 [19000/60000 (32%)]\tLoss: 1.976963\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.011589\n",
      "Train Epoch: 1 [21000/60000 (35%)]\tLoss: 2.068353\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 2.028427\n",
      "Train Epoch: 1 [23000/60000 (38%)]\tLoss: 1.881285\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 1.946727\n",
      "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 2.016184\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 1.890738\n",
      "Train Epoch: 1 [27000/60000 (45%)]\tLoss: 1.974777\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 1.910581\n",
      "Train Epoch: 1 [29000/60000 (48%)]\tLoss: 1.901539\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 1.875385\n",
      "Train Epoch: 1 [31000/60000 (52%)]\tLoss: 2.011802\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.809413\n",
      "Train Epoch: 1 [33000/60000 (55%)]\tLoss: 1.836414\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 1.865345\n",
      "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 1.898587\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 1.786371\n",
      "Train Epoch: 1 [37000/60000 (62%)]\tLoss: 1.807743\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 1.867689\n",
      "Train Epoch: 1 [39000/60000 (65%)]\tLoss: 1.802194\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 1.821832\n",
      "Train Epoch: 1 [41000/60000 (68%)]\tLoss: 1.829251\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 1.865928\n",
      "Train Epoch: 1 [43000/60000 (72%)]\tLoss: 1.847815\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 1.735321\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 1.803372\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 1.829548\n",
      "Train Epoch: 1 [47000/60000 (78%)]\tLoss: 1.855793\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.846972\n",
      "Train Epoch: 1 [49000/60000 (82%)]\tLoss: 1.841437\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 1.786132\n",
      "Train Epoch: 1 [51000/60000 (85%)]\tLoss: 1.768140\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 1.794502\n",
      "Train Epoch: 1 [53000/60000 (88%)]\tLoss: 1.775365\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 1.778196\n",
      "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 1.762549\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 1.757541\n",
      "Train Epoch: 1 [57000/60000 (95%)]\tLoss: 1.779457\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 1.645566\n",
      "Train Epoch: 1 [59000/60000 (98%)]\tLoss: 1.721976\n",
      "====> Epoch: 1 Average loss: 2.2255\n",
      "\n",
      "Epoch: 1\tTest loss: 1.774050\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/pradeep/Downloads/courses/DIP/project/submission_codes/results/epoch_1_recon.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 164\u001b[0m\n\u001b[1;32m    162\u001b[0m     train(model, device, epoch, train_loader, optimizer)\n\u001b[1;32m    163\u001b[0m     test(model, device, epoch, test_loader)\n\u001b[0;32m--> 164\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecon_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     model\u001b[38;5;241m.\u001b[39msample(device, epoch)\n\u001b[1;32m    166\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_params/torch_vae_beta_binomial_params_new\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 94\u001b[0m, in \u001b[0;36mBetaBinomialVAE.reconstruct\u001b[0;34m(self, x, device, epoch)\u001b[0m\n\u001b[1;32m     92\u001b[0m x_recon \u001b[38;5;241m=\u001b[39m x_recon\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.\u001b[39m\n\u001b[1;32m     93\u001b[0m x_with_recon \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x, x_recon))\n\u001b[0;32m---> 94\u001b[0m \u001b[43msave_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_with_recon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults/epoch_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m_recon.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dip/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dip/lib/python3.8/site-packages/torchvision/utils.py:151\u001b[0m, in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m ndarr \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39madd_(\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    150\u001b[0m im \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(ndarr)\n\u001b[0;32m--> 151\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dip/lib/python3.8/site-packages/PIL/Image.py:2563\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2561\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2562\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2563\u001b[0m         fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2565\u001b[0m     fp \u001b[38;5;241m=\u001b[39m cast(IO[\u001b[38;5;28mbytes\u001b[39m], fp)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/pradeep/Downloads/courses/DIP/project/submission_codes/results/epoch_1_recon.png'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim, lgamma\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.distributions import Normal, Categorical, Beta, Binomial\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(17)\n",
    "\n",
    "\n",
    "def beta_binomial_log_pdf(k, n, alpha, beta):\n",
    "    numer = lgamma(n+1) + lgamma(k + alpha) + lgamma(n - k + beta) + lgamma(alpha + beta)\n",
    "    denom = lgamma(k+1) + lgamma(n - k + 1) + lgamma(n + alpha + beta) + lgamma(alpha) + lgamma(beta)\n",
    "    return numer - denom\n",
    "\n",
    "\n",
    "class BetaBinomialVAE(nn.Module):\n",
    "    def __init__(self, hidden_dim=200, latent_dim=50):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.register_buffer('prior_mean', torch.zeros(1))\n",
    "        self.register_buffer('prior_std', torch.ones(1))\n",
    "        self.register_buffer('n', torch.ones(100, 784) * 255.)\n",
    "\n",
    "        self.fc1 = nn.Linear(784, self.hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(self.hidden_dim)\n",
    "\n",
    "        self.fc21 = nn.Linear(self.hidden_dim, self.latent_dim)\n",
    "        self.fc22 = nn.Linear(self.hidden_dim, self.latent_dim)\n",
    "        self.bn21 = nn.BatchNorm1d(self.latent_dim)\n",
    "        self.bn22 = nn.BatchNorm1d(self.latent_dim)\n",
    "\n",
    "        self.fc3 = nn.Linear(self.latent_dim, self.hidden_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(self.hidden_dim)\n",
    "        self.fc4 = nn.Linear(self.hidden_dim, 784*2)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Return mu, sigma on latent\"\"\"\n",
    "        h = x / 255.  # otherwise we will have numerical issues\n",
    "        h = F.relu(self.bn1(self.fc1(h)))\n",
    "        return self.bn21(self.fc21(h)), torch.exp(self.bn22(self.fc22(h)))\n",
    "\n",
    "    def reparameterize(self, mu, std):\n",
    "        if self.training:\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.bn3(self.fc3(z)))\n",
    "        h = self.fc4(h)\n",
    "        log_alpha, log_beta = torch.split(h, 784, dim=1)\n",
    "        return torch.exp(log_alpha), torch.exp(log_beta)\n",
    "\n",
    "    def loss(self, x):\n",
    "        z_mu, z_std = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(z_mu, z_std)  # sample zs\n",
    "\n",
    "        x_alpha, x_beta = self.decode(z)\n",
    "        l = beta_binomial_log_pdf(x.view(-1, 784), self.n,\n",
    "                                  x_alpha, x_beta)\n",
    "        l = torch.sum(l, dim=1)\n",
    "        p_z = torch.sum(Normal(self.prior_mean, self.prior_std).log_prob(z), dim=1)\n",
    "        q_z = torch.sum(Normal(z_mu, z_std).log_prob(z), dim=1)\n",
    "        return -torch.mean(l + p_z - q_z) * np.log2(np.e) / 784.\n",
    "\n",
    "    def sample(self, device, epoch, num=64):\n",
    "        sample = torch.randn(num, self.latent_dim).to(device)\n",
    "        x_alpha, x_beta = self.decode(sample)\n",
    "        beta = Beta(x_alpha, x_beta)\n",
    "        p = beta.sample()\n",
    "        binomial = Binomial(255, p)\n",
    "        x_sample = binomial.sample()\n",
    "        x_sample = x_sample.float() / 255.\n",
    "        save_image(x_sample.view(num, 1, 28, 28),\n",
    "                   'results/epoch_{}_samples.png'.format(epoch))\n",
    "\n",
    "    def reconstruct(self, x, device, epoch):\n",
    "        x = x.view(-1, 784).float().to(device)\n",
    "        z_mu, z_logvar = self.encode(x)\n",
    "        z = self.reparameterize(z_mu, z_logvar)  # sample zs\n",
    "        x_alpha, x_beta = self.decode(z)\n",
    "        beta = Beta(x_alpha, x_beta)\n",
    "        p = beta.sample()\n",
    "        binomial = Binomial(255, p)\n",
    "        x_recon = binomial.sample()\n",
    "        x_recon = x_recon.float() / 255.\n",
    "        x_with_recon = torch.cat((x, x_recon))\n",
    "        save_image(x_with_recon.view(64, 1, 28, 28),\n",
    "                   'results/epoch_{}_recon.png'.format(epoch))\n",
    "\n",
    "\n",
    "def train(model, device, epoch, data_loader, optimizer, log_interval=10):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch_idx, (data, _) in enumerate(data_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(data)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader),\n",
    "                loss.item()))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, np.mean(losses)))\n",
    "\n",
    "\n",
    "def test(model, device, epoch, data_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for data, _ in data_loader:\n",
    "        data = data.to(device)\n",
    "        loss = model.loss(data)\n",
    "        losses.append(loss.item())\n",
    "    print('\\nEpoch: {}\\tTest loss: {:.6f}\\n\\n'.format(\n",
    "        epoch, np.mean(losses)\n",
    "    ))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    epochs = 20\n",
    "    batch_size = 100\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "    class ToInt:\n",
    "        def __call__(self, pic):\n",
    "            return pic * 255\n",
    "\n",
    "    transforms.Compose([transforms.ToTensor(), ToInt()])\n",
    "\n",
    "    model = BetaBinomialVAE().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data/mnist', train=True, download=True,\n",
    "                       transform=transforms.Compose([transforms.ToTensor(), ToInt()])),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data/mnist', train=False, download=True,\n",
    "                       transform=transforms.Compose([transforms.ToTensor(), ToInt()])),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    recon_dataset = datasets.MNIST('data/mnist', train=False, download=True,\n",
    "                                   transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                 ToInt()])).test_data[:32]\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, epoch, train_loader, optimizer)\n",
    "        test(model, device, epoch, test_loader)\n",
    "        model.reconstruct(recon_dataset, device, epoch)\n",
    "        model.sample(device, epoch)\n",
    "    torch.save(model.state_dict(), 'saved_params/torch_vae_beta_binomial_params_new')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
